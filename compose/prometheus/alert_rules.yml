groups:
  - name: service_health
    interval: 15s
    rules:
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} has been unreachable for more than 1 minute."

  - name: api_health
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: |
          (
            sum by (job) (rate(pulse_http_requests_total{status=~"5.."}[5m]))
            /
            sum by (job) (rate(pulse_http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High HTTP 5xx error rate on {{ $labels.job }}"
          description: "{{ $labels.job }} has > 5% error rate for the last 5 minutes. Current: {{ $value | humanizePercentage }}"

      - alert: HighLatency
        expr: |
          histogram_quantile(0.95, sum by (le, job) (rate(pulse_http_request_duration_seconds_bucket[5m]))) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High p95 latency on {{ $labels.job }}"
          description: "{{ $labels.job }} p95 latency is above 2 seconds for the last 5 minutes. Current: {{ $value | humanizeDuration }}"

  - name: database
    interval: 30s
    rules:
      - alert: DBPoolExhausted
        expr: pulse_db_pool_free < 2
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "DB pool nearly exhausted on {{ $labels.service }}"
          description: "{{ $labels.service }} has fewer than 2 free DB connections for 5 minutes. Free: {{ $value }}"

  - name: auth_security
    interval: 30s
    rules:
      - alert: HighAuthFailureRate
        expr: sum(rate(pulse_auth_failures_total[5m])) * 60 > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High authentication failure rate"
          description: "More than 10 auth failures per minute for the last 5 minutes. Could indicate brute force attempt."

  - name: ingestion
    interval: 30s
    rules:
      - alert: IngestQueueBacklog
        expr: pulse_ingest_queue_depth > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Telemetry ingest queue backlog"
          description: "Ingest queue depth is {{ $value }}, indicating the ingestion pipeline is falling behind."

  - name: alert_pipeline
    interval: 30s
    rules:
      - alert: EvaluatorErrors
        expr: rate(pulse_evaluator_evaluation_errors_total[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Alert evaluator experiencing errors"
          description: "Alert rule evaluation is producing errors at {{ $value }} errors/second."

      - alert: DeliveryFailures
        expr: rate(pulse_delivery_jobs_failed_total[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Notification delivery failures"
          description: "Notification delivery is failing on channel type {{ $labels.channel_type }}."

  - name: pulse_infrastructure
    interval: 30s
    rules:
      - alert: NATSConsumerLagHigh
        expr: pulse_ingest_queue_depth > 10000
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Ingest consumer lag is {{ $value }} messages"
          description: "Ingest workers are falling behind. Consider scaling up ingest pods."

      - alert: NATSConsumerLagCritical
        expr: pulse_ingest_queue_depth > 50000
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Ingest consumer lag critical: {{ $value }} messages"

      - alert: RouteDeliveryLagHigh
        expr: pulse_route_delivery_nats_pending > 1000
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "Route delivery consumer lag: {{ $value }}"

      - alert: BatchWriteLatencyHigh
        expr: histogram_quantile(0.95, sum by (le) (rate(pulse_ingest_batch_write_seconds_bucket[5m]))) > 2
        for: 3m
        labels:
          severity: warning
        annotations:
          summary: "P95 batch write latency is {{ $value }}s"
          description: "Database writes are slow. Check PG performance and pool utilization."

      - alert: EMQXConnectionsHigh
        expr: emqx_connections_count > 50000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "EMQX has {{ $value }} active connections"
          description: "Approaching connection limits. Consider adding EMQX cluster nodes."

      - alert: DLQDepthGrowing
        expr: increase(pulse_delivery_dlq_total[1h]) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "{{ $value }} messages sent to DLQ in the last hour"
          description: "Check webhook destinations for failures."

      - alert: DBPoolSaturated
        expr: (1 - (pulse_db_pool_free / pulse_db_pool_size)) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "DB pool utilization high on {{ $labels.service }}"
          description: "Consider increasing PG_POOL_MAX or scaling the DB instance."
